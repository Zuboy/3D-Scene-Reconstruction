{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7dc1b1",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#54a7b5\">3D Computer Vision (WS25/26) - Project</h1>\n",
    "\n",
    "<h2 style=\"color:#007b90\">General Information</h2>\n",
    "\n",
    "Upload: 29.10.2025\n",
    "\n",
    "**Intermediate Deadline**: 27.11.2025 (Step 1-2)\n",
    "\n",
    "**Final Deadline**: 23.01.2026 (Step 1-5)\n",
    "\n",
    "<h3 style=\"color:#003476\">Your Group</h3>\n",
    "\n",
    "Submitted by Group XX: \n",
    "- Name1\n",
    "- Name2\n",
    "- Name3\n",
    "- Name4\n",
    "\n",
    "<h3 style=\"color:#003476\">Submission</h3>\n",
    "\n",
    "Please hand in a single **.zip** file named according to the pattern \"**groupXX**\" (e.g. group00). The contents of the .zip should be as follows:\n",
    "- folder with the same name as the **.zip** file\n",
    "    - **.ipynb** file\n",
    "    - **.html** export of .ipynb with all the outputs you got\n",
    "    - **data** folder containing necessary files to run the code\n",
    "\n",
    "By submitting this exercise, you confirm the following:\n",
    "- **All people** listed above **contributed** to this solution\n",
    "- **No other people** were **involved** in this solution\n",
    "- **No contents** of this solution were **copied from others** (this includes people, large language models, websites, etc.)\n",
    "\n",
    "<h3 style=\"color:#003476\">Final Presentation</h3>\n",
    "\n",
    "You will be required to present your solution in a 20 minute presentation, which includes:\n",
    "- Problem Overview\n",
    "- Solution Overview (e.g. pseudo code, mathematical formulas, visualizations)\n",
    "- Describe challenges & optimizations\n",
    "\n",
    "After the presentation, there will be 10 minutes of questions and answers about your work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6313394",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#007b90\">3D Scene Reconstruction - Task Overview</h2>\n",
    "\n",
    "Your task in this exercise is to do a dense reconstruction of a scene. This will involve multiple steps that you will encounter and learn about as the semester progresses. You can start implementing individual steps as soon as you learn about them or wait until you have learned more to implement everything together. In the latter case, be mindful that this exercise is designed for an entire semester and the workload is accordingly large.\n",
    "\n",
    "<h3 style=\"color:#003476\">Given Data</h3>\n",
    "\n",
    "You will be given the following data:\n",
    "- **9 color images** of the scene.\n",
    "    - 8 Bit RGB per pixel.\n",
    "    - Each image rendered from a different position.\n",
    "    - The camera used had **lens distortion**.\n",
    "- **9 Depth images** of the scene.\n",
    "    - 8 Bit Grayscale per pixel. The result of dividing the Z-depth by **each image's maximum** and then multiplying by 255.\n",
    "    - Each image has the **same pose** as the corresponding RGB image.\n",
    "    - The camera used was **free of any distortions**.\n",
    "- 1 Dictionary containing **camera calibration parameters**.\n",
    "    - They belong to the camera that was used to render the RGB images.\n",
    "    - Distortion coefficients are given in the standard [k<sub>1</sub>, k<sub>2</sub>, p<sub>1</sub>, p<sub>2</sub>, k<sub>3</sub>] order.\n",
    "- 1 Numpy array containing **8 camera transformations**.\n",
    "    - They specify the **movements** that the **camera went through** to render all images.\n",
    "    - I.e. idx **0** specifies the transformation from **00.png to 01.png**, idx **1** specifies the transformation from **01.png to 02.png**, ...\n",
    "    - This applies to both RGB and Depth images, as they have the same poses.\n",
    "- 1 Numpy array containing **7 features**.\n",
    "    - The features are specified for each of the 9 images.\n",
    "    - Each feature is a **2D pixel location in \"H, W\" order**, meaning the first value is the height/row in the image and the second width/column.\n",
    "    - If a feature was not visible, it was entered as [-1, -1].\n",
    "    - The features are **unsorted**, meaning that feature idx 0 for 00.png could be corresponding to e.g. feature idx 4 for 01.png.\n",
    "\n",
    "<h3 style=\"color:#003476\">Solution Requirements</h3>\n",
    "\n",
    "- Your code needs to **compile**, **run**, and produce an **output**.\n",
    "- Your target output should be a **dense point cloud** reconstruction (without holes) of the scene.\n",
    "    - The output should be in the **.ply format**. We provide a function that can exports a .ply file.\n",
    "    - You may inspect your .ply outputs in e.g. **Meshlab**.\n",
    "    - See the 'Dense Point Cloud' sample image to get an idea of what is possible. (Meshlab screenshot with point shading set to None; see example under data/samples)\n",
    "- Your code should be a **general solution**.\n",
    "    - This means that it could run correctly for a different dataset (with same input structure).\n",
    "    - It should **NOT** include anything **hardcoded** specific to this dataset.\n",
    "- Your code should not be unnecessarily inefficient.\n",
    "    - Our sample solution runs in less than 2 minutes total (including point cloud export).\n",
    "    - If your solution runs for more than 10 minutes, you are being wasteful in some part of your program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec86c8a",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#007b90\">Imports</h2>\n",
    "\n",
    "Please note the following:\n",
    "- These are all imports necessary to achieve the sample results.\n",
    "- You may remove and/or add other libraries at your own convinience.\n",
    "- Using library functions (from the given or other libraries) that bypass **necessary computer vision tasks** will not be recognized as 'solved'.\n",
    "    - E.g.: If you **need** to undistort an image to **get to the next step** of the solution and use the library function cv2.undistort(), then we will evaluate the **undistortion step** as '**failed**'.\n",
    "    - E.g.: If you **want** to draw points in an image (to **check your method** or **visualize in-between steps**) and use the library function cv2.circle(), then there is **no problem**.\n",
    "    - E.g.: If you **need** to perform complex **mathematical** operations and use some numpy function, then there is **no problem**.\n",
    "    - E.g.: You do not like a **provided utility function** and find/know a library function that gives the **same outputs** from the **same inputs**, then there is **no problem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b8dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#make plots interactive:\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68e1e4",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#007b90\">Data Preparation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85509287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The project's root path is '/home/bendig/Desktop/projects/3dcv/project'.\n",
      "Reading data from '/home/bendig/Desktop/projects/3dcv/project/data'.\n",
      "Image folder: '/home/bendig/Desktop/projects/3dcv/project/data/images'.\n",
      "Depth folder: '/home/bendig/Desktop/projects/3dcv/project/data/depths'.\n",
      "\n",
      "Creating directory '/home/bendig/Desktop/projects/3dcv/project/output'.\n",
      "Creating directory '/home/bendig/Desktop/projects/3dcv/project/output/point_cloud'.\n",
      "\n",
      "Camera Calibration:\n",
      "  distortion_param: [-0.1, 0.02, 0.0, 0.0, -0.01]\n",
      "  image_height: 551\n",
      "  image_width: 881\n",
      "  principal_point: [275.0, 440.0]\n",
      "  focal_length_mm: 25\n",
      "  sensor_width_mm: 35\n",
      "  pixel_ratio: 1.0\n",
      "  pixel_per_mm: 25.17142857142857\n",
      "  focal_length_px: 629.2857142857142\n",
      "Camera Movement: (8, 4, 4)\n",
      "2D Features (Unsorted): (9, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "#Inputs\n",
    "base_path = os.getcwd()\n",
    "data_path = os.path.join(base_path, f\"data\")\n",
    "img_path = os.path.join(data_path, 'images')\n",
    "depth_path = os.path.join(data_path, 'depths')\n",
    "print(f\"The project's root path is '{base_path}'.\")\n",
    "print(f\"Reading data from '{data_path}'.\")\n",
    "print(f\"Image folder: '{img_path}'.\")\n",
    "print(f\"Depth folder: '{depth_path}'.\")\n",
    "\n",
    "#Outputs\n",
    "out_path = os.path.join(base_path, 'output')\n",
    "ply_path = os.path.join(out_path, 'point_cloud')\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "os.makedirs(ply_path, exist_ok=True)\n",
    "print(f\"\\nCreating directory '{out_path}'.\")\n",
    "print(f\"Creating directory '{ply_path}'.\")\n",
    "\n",
    "#Load Data\n",
    "camera_calibration = np.load(os.path.join(data_path, 'camera_calibration.npy'), allow_pickle=True)\n",
    "camera_calibration = camera_calibration.item()#get dictionary from numpy array struct\n",
    "given_features = np.load(os.path.join(data_path, 'given_features.npy'), allow_pickle=True)\n",
    "camera_movement = np.load(os.path.join(data_path, 'camera_movement.npy'), allow_pickle=True)\n",
    "\n",
    "print(f\"\\nCamera Calibration:\")\n",
    "for entry in camera_calibration.items():\n",
    "    print(f\"  {entry[0]}: {entry[1]}\")\n",
    "print(f\"Camera Movement: {camera_movement.shape}\")#[Cameras-1, 4, 4]\n",
    "print(f\"2D Features (Unsorted): {given_features.shape}\")#[Camera_idx, Feature_idx, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131f010",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#007b90\">Provided Utility Functions</h2>\n",
    "\n",
    "These functions are provided to reduce the complexity of some steps you might encounter. They were involved in the creation of the given samples. However, you do not have to use them and can use other means of achieving the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49dea259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(numpy_image, numpy_sample_grid):\n",
    "    '''\n",
    "    This function samples a target image from a source image (numpy_image) based on specified pixel coordinates (numpy_sample_grid).\n",
    "    Inputs:\n",
    "        numpy_image: of shape=[H, W, C]. H is the height, W is the width, and C is the color channel of the source image from which color values will be sampled.\n",
    "        numpy_sample_grid: of shape=[H, W, UV]. H is the height and W is the width of the target image that will be sampled. UV are the pixel locations in the source image from which to sample color values.\n",
    "    Outputs:\n",
    "        sampled_image: of shape=[H, W, C]. H is the height, W is the width, and C is the color channel of the target image that was sampled.\n",
    "    \n",
    "    The following is a simple toy example to see the behavior of the sample_image function\n",
    "\n",
    "    example_source = (np.random.rand(110, 220, 3)*255).astype(int) #[110, 220, 3]\n",
    "    example_grid = np.ones([100, 200, 2]) #[100, 200, 2]\n",
    "    example_grid[:, :, 1] = 2\n",
    "    example_target = sample_image(example_source, example_grid) #[100, 200, 3]\n",
    "\n",
    "    example_target will be of shape [100, 200, 3]\n",
    "    -> 100, because example_grid has a height of 100\n",
    "    -> 200, because example_grid has a width of 200\n",
    "    -> 3, because example_source has a color channel of 3\n",
    "    print(example_source.shape)\n",
    "\n",
    "    example_target will contain the value of example_source[2, 1] at every pixel\n",
    "    -> 2, because example_grid[:, :, 1] has a value of 2 for every pixel\n",
    "    -> 1, because example_grid[:, :, 0] has a value of 1 for every pixel\n",
    "    print(example_source[2, 1, 0], \"->\", np.unique(example_target[:, :, 0]))\n",
    "    print(example_source[2, 1, 1], \"->\", np.unique(example_target[:, :, 1]))\n",
    "    print(example_source[2, 1, 2], \"->\", np.unique(example_target[:, :, 2]))\n",
    "    \n",
    "    '''\n",
    "    height, width, _ = numpy_image.shape#[H, W, 3]\n",
    "\n",
    "    #turn numpy array to torch tensor\n",
    "    torch_sample_grid = torch.from_numpy(numpy_sample_grid)#[H, W, 2]\n",
    "    #normalize from range (0, width-1) to (0, 1)\n",
    "    torch_sample_grid[:, :, 0] = torch_sample_grid[:, :, 0] / (width-1)\n",
    "    #normalize from range (0, height-1) to (0, 1)\n",
    "    torch_sample_grid[:, :, 1] = torch_sample_grid[:, :, 1] / (height-1)\n",
    "    #normalize from range (0, 1) to (-1, 1)\n",
    "    torch_sample_grid = torch_sample_grid*2 -1\n",
    "\n",
    "    #transform to necessary shapes\n",
    "    torch_sample_grid = torch_sample_grid.unsqueeze(0)#[1, H, W, 2]\n",
    "    torch_image = torch.from_numpy(numpy_image).double().permute(2, 0, 1).unsqueeze(0)#[1, 3, H, W]\n",
    "    #sample image according to sample grid locations from source image\n",
    "    sampled_image = torch.nn.functional.grid_sample(torch_image, torch_sample_grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "    #transform back to numpy image\n",
    "    sampled_image = sampled_image.squeeze().permute(1, 2, 0).numpy().astype(np.uint8)#[H, W, 3]\n",
    "    return sampled_image\n",
    "\n",
    "def ply_creator(input_3d, rgb_data=None, filename='dummy'):\n",
    "    ''' Creates a colored point cloud that you can visualise using e.g. Meshlab.\n",
    "    Inputs:\n",
    "        input_3d: of shape=[N, 3], each row is the 3D coordinate of each point\n",
    "        rgb_data(optional): of shape=[N, 3], each row is the rgb color value of each point\n",
    "        filename: file name for the .ply file to be created \n",
    "    '''\n",
    "    assert (input_3d.ndim==2),\"Pass 3d points as NumPointsX3 array \"\n",
    "    pre_text1 = \"\"\"ply\\nformat ascii 1.0\"\"\"\n",
    "    pre_text2 = \"element vertex \"\n",
    "    pre_text3 = \"\"\"property float x\\nproperty float y\\nproperty float z\\nproperty uchar red\\nproperty uchar green\\nproperty uchar blue\\nend_header\"\"\"\n",
    "    pre_text22 = pre_text2 + str(input_3d.shape[0])\n",
    "    pre_text11 = pre_text1\n",
    "    pre_text33 = pre_text3\n",
    "    filename = filename + '.ply'\n",
    "    fid = open(filename, 'w')\n",
    "    fid.write(pre_text11)\n",
    "    fid.write('\\n')\n",
    "    fid.write(pre_text22)\n",
    "    fid.write('\\n')\n",
    "    fid.write(pre_text33)\n",
    "    fid.write('\\n')\n",
    "    for i in range(input_3d.shape[0]):\n",
    "        for c in range(3):\n",
    "            fid.write(str(input_3d[i,c]) + ' ')\n",
    "        if not rgb_data is None:\n",
    "            for c in range(3):\n",
    "                fid.write(str(rgb_data[i,c]) + ' ')\n",
    "        if i!=input_3d.shape[0]:\n",
    "            fid.write('\\n')\n",
    "    fid.close()\n",
    "    return filename "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9eb1cf",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#007b90\">Step 1: Undistortion of the RGB Images</h2>\n",
    "\n",
    "As described in the Given Data section, the RGB images contain lens distortion, while the depth images are already undistorted. To ensure accurate 3D reconstruction, the two modalities must be aligned.\n",
    "\n",
    "Undistort all RGB images, then visualize one example image before and after undistortion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c469535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the necessary code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb22a2e",
   "metadata": {},
   "source": [
    "Example visualization\n",
    "\n",
    "![title](data/samples/image_distortion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda6f29",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#007b90\">Step 2: Common Camera Reference Frame</h2>\n",
    "\n",
    "The provided camera poses are expressed relative to consecutive frames (e.g. the first pose is a transformation from the first view to the second view). However, the final point cloud needs to be definied with respect to a single, common world coordinate system.\n",
    "\n",
    "Compute camera poses such that they are expressed in terms of a shared world reference frame, and then visualize all camera positions and orientations in a single 3D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a713599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the necessary code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b864a",
   "metadata": {},
   "source": [
    "Example visualization\n",
    "\n",
    "![title](data/samples/cameras.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9377be",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#007b90\">Step 3: Feature Matching and Depth Map Scaling</h2>\n",
    "\n",
    "Each depth map is scaled differently according to its own maximum value. To obtain a consistent dense point cloud, we must correct these scalings so that 3D points from different views align properly.\n",
    "\n",
    "Perform feature matching across the images, and use the corresponding matches to determine the correct scale factors for the depth maps.\n",
    "Additionally, visualize one selected feature across all views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f1ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the necessary code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b47f7",
   "metadata": {},
   "source": [
    "Example visualization of the 7 features\n",
    "\n",
    "![title](data/samples/00_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9675541",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#007b90\">Step 4: Depth Map to 3D Points</h2>\n",
    "\n",
    "Using the rescaled depth maps together with the camera poses from Step 2, compute the 3D coordinates for all pixel in each RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0874ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the necessary code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf032b2",
   "metadata": {},
   "source": [
    "Example visualization of cameras and 3D points (of the 7 features) from one image\n",
    "\n",
    "![title](data/samples/features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac6fc4",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#007b90\">Step 5: 3D Points to Point Cloud</h2>\n",
    "\n",
    "Finally, use the provided `ply_creator` function to generate a merged colored point cloud from the computed 3D points from all images. Visualize the resulting point cloud in a viewer such as MeshLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ca74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the necessary code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b08404",
   "metadata": {},
   "source": [
    "Example visualization\n",
    "\n",
    "![title](data/samples/dense_point_cloud.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dcv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
